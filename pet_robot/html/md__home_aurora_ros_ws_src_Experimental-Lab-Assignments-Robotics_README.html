<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Experimental-Lab-Assignments-Robotics: Behavioral Architecture</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Experimental-Lab-Assignments-Robotics
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Behavioral Architecture </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The scenario describes a behavior simulation of a pet robot that interacts with humans and moves in a discrete 2D environment. The human can interact with the robot through speech comands and pointing gestures while the robot can have three beaviors: sleep, play, normal. At the beginning, the robot is in a sleep state. Everytime the robot is in the sleep state, it reaches the home position and after some time switches to normal state. Everytime the robot is in the normal state, it moves randomly and reaches three different positions, then it listens to user's comands. If user indicates a position to reach, it takes some time to reach the position and change the state into 'sleep' or 'play' with a random choice. If user tells him to go to play or to sleep, it respectively changes state into 'play' or 'sleep'. When the state is 'play' the robot reaches person's position, after that it waits for a pointing gesture, if it receives the target position, it reaches this point. After some time, the robot switches to the 'normal' state.</p>
<h2>ROS Architecture of the System</h2>
<p>The system is made by three ros nodes: "sim_perception.py", "pet_state_machine.py" and "display_position.py". The rqt_graph is shown below.</p>
<h1>Rqt_graph</h1>
<p>&lt;img src=<a href="https://github.com/au1698/Experimental-Lab-Assignments-Robotics/blob/main/pet_robot/Images/rqt_graph_pet_robot.png">https://github.com/au1698/Experimental-Lab-Assignments-Robotics/blob/main/pet_robot/Images/rqt_graph_pet_robot.png</a> raw=true"&gt; </p>
<h2>sim_perception</h2>
<p>This node until is active simulates user's random choice between a pointing gesture (2D coordinates generation) and the vocal comands: 'go_to_home' and 'play'. It prints on the screen user's choice. Pointed gesture is of type 'Int64MultiArray' while vocal comands are simply of the type 'string'. It checks if "user_comand" is a string or an array. In the first case "user_comand" data are published on the topic /pointed_comand in the second data are published onthe topic /vocal_comand.</p>
<h2>pet_state_machine</h2>
<p>This node is a finite state machine composed of three states: PLAY, SLEEP, NORMAL.</p>
<p>&lt;img src=<a href="https://github.com/au1698/Experimental-Lab-Assignments-Robotics/blob/main/pet_robot/Images/pet_state_machine.jpeg">https://github.com/au1698/Experimental-Lab-Assignments-Robotics/blob/main/pet_robot/Images/pet_state_machine.jpeg</a> raw=true"&gt; </p>
<p>SLEEP: the state publishes on the topic '/target_point' the home position that the robot should reaches. If the robot is in the 'SLEEP' state and receives the comand 'go_to_home', it notifiess that is already at home.</p>
<p>NORMAL: the state publishes on the topic '/target_point' three random target positions, after that it subscribes to the topic '/vocal_comand' and '/pointed_comand'. If it receives a pointed comand, it publishes it on the topic '/target_position' and then it changes the state choosing randomly from 'go_to_sleep' or 'go_to_play'. Otherwise it checks if the vocal comand is 'go_to_home' switches into the 'SLEEP' state if it is 'play' switches into the 'PLAY' state.</p>
<p>PLAY: the state publishes on the topic '/target_point' a random position which represents person's position. The state takes time to reach person's position and wait for a pointing gesture. It subscribes to the topic '/vocal_comand', if it receives user's vocal comands it return and error because in this wait loop expects a pointing gesture. If it doesn't receive a vocal comand it subscribes to the topic '/pointed_comand' , it publishes on the topic '/target_point' and exit from the loop.</p>
<h2>display_position</h2>
<p>This node subscribes to the topic '/target_point' and display when the robot arrives to the target.</p>
<h2>How to run the code</h2>
<p>The first thing to do, after having cloned the repository in the Ros workspace, is to build the package in your workspace with ``` catkin_make ``` and give running permissions to it with ``` $ chmod +x ```</p>
<p>To run the system: </p><pre class="fragment">```
roslaunch pet_robot pet_robot.launch

```
</pre><p> To visualize the Smach Viewer: </p><pre class="fragment">```
</pre><p> rosrun smach_viewer smach_viewer.py</p>
<p>```</p>
<h2>Working hypoteses</h2>
<p>The gesture commands present the same "priority" since they occur in random order. The home position is fixed (0,0). Person's position is generated randomly in 'PLAY' state. There is not a simulator. When the robot is in the 'SLEEP' state, it only reaches the home position and after some time goes to the 'NORMAL' state, even if receives as user's comand: 'play' or a pointed gesture.</p>
<h2>Possible improvements</h2>
<p>Use the ROS parameter service to define a parameter to scale the simulation velocity. Using a service-client as kind of comunication between the simulation node and the state machine in order to improve the synchronization.</p>
<h2>Author:</h2>
<ul>
<li>Aurora Bertino: <a href="#" onclick="location.href='mai'+'lto:'+'ber'+'ti'+'no.'+'au'+'ror'+'a1'+'6@g'+'ma'+'il.'+'co'+'m'; return false;">berti<span style="display: none;">.nosp@m.</span>no.a<span style="display: none;">.nosp@m.</span>urora<span style="display: none;">.nosp@m.</span>16@g<span style="display: none;">.nosp@m.</span>mail.<span style="display: none;">.nosp@m.</span>com</a> </li>
</ul>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
